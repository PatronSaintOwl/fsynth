## Concept

Fragment provide an unique bitmap-based organic approach to sound synthesis and composition with the capability to generate audio and visuals in real-time and at the same time.

A vast range of sounds can be created with complete control over harmonic content and spatial dynamics.

### Visuals

Fragment canvas is the source of the visuals and also the source of the sound synthesis.

Visuals are generated by giving instructions to the GPU with a simple high-level programming language called [GLSL](https://en.wikipedia.org/wiki/OpenGL_Shading_Language), the Fragment GLSL script (also called a *fragment* shader) is called by the GPU for every pixels on the canvas. This approach allow very fast GPU-only real-time manipulation of the bitmap data.

Fragment has only one fragment shader which has the particularity to be shared between the users of an online session, it update and compile on-the-fly as you or other peoples type, this is the collaborative nature of Fragment, visuals and sounds can be produced online, some session settings are also synchronized between users such as slices.

GLSL is a powerful language but has some hard constraints due to its GPU-only nature, this is why Fragment also support the [Processing](https://en.wikipedia.org/wiki/Processing_(programming_language)) language which is a general purpose programming language associated with a simple and powerful graphical library.

### Sound Synthesis

Fragment is a so-called 'image-synth', the sound synthesis data source is entirely bitmap-based.

With Fragment, all sounds is operated from a frequency level by drawing algorithmically or by the mouse over a canvas, frequency is represented on the vertical axis and stereo/mono amplitude is represented by the intensity of each pixels, parts of the canvas is then captured by user-positioned slices.

Since the sound synthesis data source is bitmap-based, we need a way to capture the pixels data, this is done by slicing the canvas vertically, pixels data (1px wide) are then captured from the slice at the browser display refresh rate and are then translated to notes from the RGBA pixels value, the notes are then interpreted and played by one or more synthesis method in real-time.

The canvas represent frequencies (exponential map) on the vertical axis, the horizontal axis generally represent time.

One of the unique feature of Fragment is the time visualization of sounds, the horizontal axis span several pixels enabling the user to see a sound past, present and future.

There is many type of sound synthesis available within Fragment, all work from the same bitmap-based concept.

#### Additive Synthesis

> **Additive synthesis** is a sound synthesis technique that creates [timbre](https://en.wikipedia.org/wiki/Timbre) by adding [sine](https://en.wikipedia.org/wiki/Sine) waves together.

Fragment default sound synthesis is additive, with additive synthesis, Fragment become a fully capable spectral synthesizer able to do re-synthesis based on imported images, videos or audio files, any number of partials can be produced, there is no limits except the canvas height and the computing resources available.

#### Granular Synthesis

> **Granular synthesis** is a sound synthesis technique that operates on the microsound time scale, it is based on the same principle as [sampling](https://en.wikipedia.org/wiki/Sampling_(music)).

Fragment secondary sound synthesis is granular, the grains source are based on audio samples, this method is only available with the audio server and provide asynchronous and synchronous granular synthesis.

Just like additive synthesis, re-synthesis can be done with granular synthesis and most granular parameters can be manipulated by the user.

The combination of granular synthesis and additive synthesis provide powerful sound capabilities.

#### Subtractive Synthesis

> Subtractive synthesis start from harmonically rich waveforms which are then filtered.

Subtractive synthesis is only available with the audio server, it use anti-aliased PolyBLEP waveforms and can also use additive synthesis (much slower).

There is three type of band-limited (no aliasing!) waveforms : sawtooth, square, triangle

There is also a noise waveform.

Subtractive synthesis use a high quality low-pass filter (Moog type).

#### Phase Modulation

> Phase modulation (PM) is a mean to generate sounds by modulating the phase of an oscillator (carrier) from another oscillator (modulator), it is very similar to frequency modulation (FM).

PM synthesis in Fragment work by giving an oscillator index (based on image-height) to pixel, this oscillator will be used as a modulator.

PM synthesis use a high quality low-pass filter (Moog type).

Only available with the audio server.

#### Physical modelling

> Physical modelling synthesis refers to sound synthesis methods in which the waveform of the sound to be generated is computed using a mathematical model, a set of equations and algorithms to simulate a physical source of sound, usually a musical instrument.

Physical modelling in Fragment use the Karplus-Strong string synthesis algorithm.

Only available with the audio server.

#### Wavetable

> Wavetable synthesis is a sound synthesis technique that employs arbitrary periodic waveforms in the production of musical tones or notes.

Wavetable synthesis use user-defined single cycle waveforms / samples.

Only available with the audio server.

#### Sampler

Fragment can also act like a regular sampler through granular synthesis method.